{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading Corpus from NLTK\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "\n",
    "# Load the Treebank corpus\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "tagged_words = [ tup for sent in nltk_data for tup in sent ]\n",
    "\n",
    "tags = {tag for word,tag in tagged_words}\n",
    "vocab = {word for word,tag in tagged_words}\n",
    "\n",
    "# compute Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "     \n",
    "    return (count_w_given_tag, count_tag)\n",
    "\n",
    "# compute Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)\n",
    "\n",
    "# creating t x t transition matrix of tags, t= no of tags\n",
    "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
    " \n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    " \n",
    "def Viterbi(words, train_bag = tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the movie_reviews dataset if not already downloaded\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\NLP\\Sentiment-Analysis\\Improved_Sentiment_Analyzer.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         combined_features\u001b[39m.\u001b[39mappend(combined_feature)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(combined_features)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m X_train_combined \u001b[39m=\u001b[39m combine_features(X_train_tfidf, X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m X_val_combined \u001b[39m=\u001b[39m combine_features(X_val_tfidf, X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m X_test_combined \u001b[39m=\u001b[39m combine_features(X_test_tfidf, X_test)\n",
      "\u001b[1;32mc:\\Users\\user\\NLP\\Sentiment-Analysis\\Improved_Sentiment_Analyzer.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m tfidf_vector \u001b[39m=\u001b[39m tfidf_vectors[i]\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Calculate POS tag features for the sentence\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m pos_tags \u001b[39m=\u001b[39m Viterbi(sentences[i]\u001b[39m.\u001b[39msplit())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Calculate average TF-IDF weight for words associated with each POS tag\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m avg_tfidf_per_tag \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32mc:\\Users\\user\\NLP\\Sentiment-Analysis\\Improved_Sentiment_Analyzer.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     transition_p \u001b[39m=\u001b[39m tags_df\u001b[39m.\u001b[39mloc[state[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], tag]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# compute emission and state probabilities\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m emission_p \u001b[39m=\u001b[39m word_given_tag(words[key], tag)[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mword_given_tag(words[key], tag)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m state_probability \u001b[39m=\u001b[39m emission_p \u001b[39m*\u001b[39m transition_p    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m p\u001b[39m.\u001b[39mappend(state_probability)\n",
      "\u001b[1;32mc:\\Users\\user\\NLP\\Sentiment-Analysis\\Improved_Sentiment_Analyzer.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_given_tag\u001b[39m(word, tag, train_bag \u001b[39m=\u001b[39m tagged_words):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     tag_list \u001b[39m=\u001b[39m [pair \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m train_bag \u001b[39mif\u001b[39;00m pair[\u001b[39m1\u001b[39m]\u001b[39m==\u001b[39mtag]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     count_tag \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tag_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     w_given_tag_list \u001b[39m=\u001b[39m [pair[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m tag_list \u001b[39mif\u001b[39;00m pair[\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39mword]\n",
      "\u001b[1;32mc:\\Users\\user\\NLP\\Sentiment-Analysis\\Improved_Sentiment_Analyzer.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_given_tag\u001b[39m(word, tag, train_bag \u001b[39m=\u001b[39m tagged_words):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     tag_list \u001b[39m=\u001b[39m [pair \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m train_bag \u001b[39mif\u001b[39;00m pair[\u001b[39m1\u001b[39m]\u001b[39m==\u001b[39mtag]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     count_tag \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tag_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/NLP/Sentiment-Analysis/Improved_Sentiment_Analyzer.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     w_given_tag_list \u001b[39m=\u001b[39m [pair[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m tag_list \u001b[39mif\u001b[39;00m pair[\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39mword]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the movie_reviews corpus\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents to ensure randomness\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Extract the text and labels\n",
    "reviews = [\" \".join(document) for document, category in documents]\n",
    "labels = [category for _, category in documents]\n",
    "\n",
    "# Split the data into train, validation, and test sets (70-15-15 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(reviews, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Combine POS tag features with TF-IDF embeddings\n",
    "def combine_features(tfidf_vectors, sentences):\n",
    "    combined_features = []\n",
    "    for i in range(len(sentences)):\n",
    "        # Extract TF-IDF vector for the sentence\n",
    "        tfidf_vector = tfidf_vectors[i].toarray()\n",
    "\n",
    "        # Calculate POS tag features for the sentence\n",
    "        pos_tags = Viterbi(sentences[i].split())\n",
    "        \n",
    "        # Calculate average TF-IDF weight for words associated with each POS tag\n",
    "        avg_tfidf_per_tag = {}\n",
    "        for word, tag in pos_tags:\n",
    "            if tag not in avg_tfidf_per_tag:\n",
    "                avg_tfidf_per_tag[tag] = []\n",
    "            word_index = tfidf_vectorizer.vocabulary_.get(word, -1)\n",
    "            if word_index != -1:\n",
    "                avg_tfidf_per_tag[tag].append(tfidf_vector[0][word_index])\n",
    "        \n",
    "        # Calculate total count of each POS tag in the sentence\n",
    "        tag_counts = {tag: pos_tags.count(tag) for word, tag in pos_tags}\n",
    "        \n",
    "        # Calculate mean TF-IDF for each tag\n",
    "        mean_tfidf_per_tag = {tag: np.mean(weights) if weights else 0.0 for tag, weights in avg_tfidf_per_tag.items()}\n",
    "        \n",
    "        # Combine calculated features\n",
    "        combined_feature = np.concatenate([tfidf_vector, np.array([list(tag_counts.values())]), np.array([list(mean_tfidf_per_tag.values())])])\n",
    "        combined_features.append(combined_feature)\n",
    "    \n",
    "    return np.array(combined_features)\n",
    "\n",
    "X_train_combined = combine_features(X_train_tfidf, X_train)\n",
    "X_val_combined = combine_features(X_val_tfidf, X_val)\n",
    "X_test_combined = combine_features(X_test_tfidf, X_test)\n",
    "\n",
    "# Train the same classifier on the new features\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict\n",
    "test_predictions = naive_bayes_classifier.predict(X_test_combined)\n",
    "\n",
    "# Evaluate\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(classification_report(y_test, test_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
